{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13011054,"sourceType":"datasetVersion","datasetId":8237369},{"sourceId":13011085,"sourceType":"datasetVersion","datasetId":8237393},{"sourceId":13017848,"sourceType":"datasetVersion","datasetId":8241949},{"sourceId":13018817,"sourceType":"datasetVersion","datasetId":8242635}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"A spatial proteomics simplefied pipeline for stain-to-stain translation with leave-one-marker-out: \n\n*   Inputs: multiplexed tissue images with varying markers panels. Data was uploaded to Kaggle.  \n*   Preprocessing: normalize intensities by regressing each channel on chromatin (DNA and core histones) markers and compute residuals, then apply PCA to project residuals into a 3-channel representation compatible with Resnet50. \n*   Encoder: pretrained Resnet50 (partially frozen) to extract morphological features. \n*   Decoder: U-Net with bilinear upsampling to reconstruct the left-out marker/channel.","metadata":{}},{"cell_type":"markdown","source":"# imports","metadata":{}},{"cell_type":"code","source":"!pip install pytorch_msssim\n!pip install imagecodecs\n!pip install torchmetrics\n!pip install clean-fid\n#!pip install torch-fidelity","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-28T09:39:54.048823Z","iopub.execute_input":"2025-09-28T09:39:54.049032Z","iopub.status.idle":"2025-09-28T09:41:19.743724Z","shell.execute_reply.started":"2025-09-28T09:39:54.049005Z","shell.execute_reply":"2025-09-28T09:41:19.742840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport tifffile\nimport zipfile\nimport imagecodecs\nimport tempfile\nimport pickle\nimport imageio\nimport sys\nimport psutil\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport gc\nimport time\nimport glob\nfrom collections import Counter, OrderedDict\nimport cv2\nimport random\nfrom scipy.stats import pearsonr\nfrom sklearn.decomposition import PCA, IncrementalPCA\nfrom sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.parametrizations import weight_norm\nfrom torch.utils.data import TensorDataset, DataLoader, random_split, Dataset\nfrom torch.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torchvision import models\nimport skimage.transform\nfrom pytorch_msssim import ssim\nfrom tqdm import tqdm\nimport torchvision.utils as vutils\nfrom cleanfid import fid\n#from torchmetrics.image.fid import FrechetInceptionDistance\n\n\nimport matplotlib.pyplot as plt\nparams = {'axes.titlesize': 30,\n          'legend.fontsize': 16,\n          'figure.figsize': (16, 10),\n          'axes.labelsize': 16,\n          'axes.titlesize': 12,\n          'xtick.labelsize': 16,\n          'ytick.labelsize': 16,\n          'figure.titlesize': 30}\n\nplt.rcParams.update(params)\nplt.style.use('seaborn-v0_8-whitegrid')","metadata":{"execution":{"iopub.status.busy":"2025-09-26T08:04:14.755154Z","iopub.execute_input":"2025-09-26T08:04:14.755881Z","iopub.status.idle":"2025-09-26T08:04:14.878078Z","shell.execute_reply.started":"2025-09-26T08:04:14.755856Z","shell.execute_reply":"2025-09-26T08:04:14.877546Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#print(device)\n#print(torch.cuda.get_device_name(0))\n#print(f\"Allocated: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB\")\n#print(f\"Cached: {torch.cuda.memory_reserved(0)/1024**2:.2f} MB\")\n#print(f\"Max allocated: {torch.cuda.max_memory_allocated(0)/1024**2:.2f} MB\")\n#!free -h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T07:57:24.096725Z","iopub.status.idle":"2025-09-26T07:57:24.097009Z","shell.execute_reply.started":"2025-09-26T07:57:24.096860Z","shell.execute_reply":"2025-09-26T07:57:24.096876Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# functions","metadata":{}},{"cell_type":"code","source":"def save_images_to_folder(images, folder):\n    for i, img in enumerate(images):\n        path = os.path.join(folder, f\"{i}.png\")\n        imageio.imwrite(path, img)\n\ndef to_hwc(img):\n    '''\n    Different libraries use different conventions.\n    - PyTorch: prefers CHW format (channels, height, width)\n    - TensorFlow/PIL/Matplotlib: prefer HWC format (height, width, channels)\n    The function ensures all images are consistently in HWC format for the preprocessing pipeline, regardless of how they were originally stored or loaded.\n    '''\n    if img.ndim == 2:\n        img = img[..., None]\n    if img.shape[0] < img.shape[1] and img.shape[0] < img.shape[2]:\n        img = np.transpose(img, (1, 2, 0))\n    return img.astype(np.float32, copy=False)\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef load_model(model, path):\n    state_dict = torch.load(path)\n    # If data parallel wrapper, keys start with 'module.'\n    if list(state_dict.keys())[0].startswith(\"module.\"):\n        # Remove 'module.' prefix from all keys:\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            new_key = k.replace(\"module.\", \"\")\n            new_state_dict[new_key] = v\n        model.load_state_dict(new_state_dict)\n    else:\n        model.load_state_dict(state_dict)\n    return model\n\ndef fit_preprocessing(\n    train_files,\n    target_idx,\n    control_markers_indices,\n    n_components=3,\n    batch_size=100_000,\n    cofactor=5.0,\n    eps=1e-8,\n):\n    \"\"\"\n    Fit channel-wise regressions and incremental PCA on residuals from IMC training images with lazy loading.\n    Returns: (regression_models, pca, pca_mean, pca_std, input_channels)\n    \"\"\"\n\n    # Discover channel setup from the first image\n    input_channels = []\n    img0 = to_hwc(tifffile.imread(train_files[0]))\n    print(f\"......example image shape: {img0.shape}\")\n    H, W, C = img0.shape\n    input_channels = [c for c in range(C) if c != target_idx and c not in control_markers_indices]\n    K = len(input_channels)\n    del img0; gc.collect()\n\n    if not input_channels:\n        print(f\"Warning: no inputs for target {target_idx} with controls {control_markers_indices}.\")\n        return None, None, None, None, input_channels\n\n    # 1. Fit regressions with SGDRegressor (partial_fit: true \"streaming\" mode)\n    regression_models = {}\n    first_pass = {}\n\n    for j in input_channels:\n        regression_models[j] = SGDRegressor(max_iter=1000, tol=1e-3)\n        first_pass[j] = True\n\n    for c, f in enumerate(train_files):\n        if c % int(len(train_files)/2) == 0 or c == len(train_files)-1:\n            print(f\"......extracting control and trg markers: {c+1} / {len(train_files)}\")\n        img = to_hwc(tifffile.imread(f))\n        Xc = img[..., control_markers_indices].reshape(-1, len(control_markers_indices))\n        for j in input_channels:\n            y = img[..., j].reshape(-1)\n            if first_pass[j]:\n                regression_models[j].partial_fit(Xc, y)\n                first_pass[j] = False\n            else:\n                regression_models[j].partial_fit(Xc, y)\n        del img, Xc, y; gc.collect()\n\n    # 2. Incremental PCA on residuals (streamed, as before)\n    if n_components > K:\n        print(f\"n_components {n_components} > residual dim {K}; using {K}.\")\n        n_components = K\n    pca = IncrementalPCA(n_components=n_components)\n\n    for c, f in enumerate(train_files):\n        if c % int(len(train_files)/2) == 0 or c == len(train_files):\n            print(f\"......fitting regression: {c+1} / {len(train_files)}\")\n\n        img = to_hwc(tifffile.imread(f))\n        Xc = img[..., control_markers_indices].reshape(-1, len(control_markers_indices))\n        R = np.empty((Xc.shape[0], K), dtype=np.float32)\n        for k, j in enumerate(input_channels):\n            y = img[..., j].reshape(-1)\n            y_pred = regression_models[j].predict(Xc)\n            R[:, k] = y - y_pred.astype(np.float32, copy=False)\n        R = np.arcsinh(R / cofactor).astype(np.float32)\n        for s in range(0, R.shape[0], batch_size):\n            batch = R[s : s + batch_size]\n            pca.partial_fit(batch)\n        del img, Xc, R; gc.collect()\n\n    # 3. Compute mean/std of PCA space (streamed)\n    total = 0\n    sum_z = np.zeros(n_components, dtype=np.float64)\n    sumsq_z = np.zeros(n_components, dtype=np.float64)\n\n    for c, f in enumerate(train_files):\n        if c % int(len(train_files)/2) == 0 or c == len(train_files):\n            print(f\"......normalization: {c+1} / {len(train_files)}\")\n\n        img = to_hwc(tifffile.imread(f))\n        Xc = img[..., control_markers_indices].reshape(-1, len(control_markers_indices))\n        R = np.empty((Xc.shape[0], K), dtype=np.float32)\n        for k, j in enumerate(input_channels):\n            y = img[..., j].reshape(-1)\n            y_pred = regression_models[j].predict(Xc)\n            R[:, k] = y - y_pred.astype(np.float32, copy=False)\n        R = np.arcsinh(R / cofactor).astype(np.float32)\n        for s in range(0, R.shape[0], batch_size):\n            Z = pca.transform(R[s : s + batch_size]).astype(np.float32)\n            sum_z += Z.sum(axis=0)\n            sumsq_z += (Z * Z).sum(axis=0)\n            total += Z.shape[0]\n        del img, Xc, R, Z; gc.collect()\n\n    if total == 0:\n        print(\"Warning: no residuals seen for PCA stats; using zeros/ones.\")\n        pca_mean = np.zeros(n_components, dtype=np.float32)\n        pca_std = np.ones(n_components, dtype=np.float32) * eps\n    else:\n        pca_mean = (sum_z / total).astype(np.float32)\n        var = (sumsq_z / total) - (pca_mean.astype(np.float64) ** 2)\n        pca_std = (np.sqrt(np.maximum(var, 0.0)) + eps).astype(np.float32)\n\n    # Print variance captured by PCs if possible\n    if hasattr(pca, \"explained_variance_ratio_\") and pca.explained_variance_ratio_ is not None:\n        pct = pca.explained_variance_ratio_[:n_components].sum() * 100.0\n        print(f\"......PCA projection to {n_components}D captures {pct:.2f}% variance\")\n\n    return regression_models, pca, pca_mean, pca_std, input_channels\n\ndef preprocess_image(\n    file_path,\n    target_idx,\n    control_markers_indices,\n    regression_models,\n    pca,\n    pca_mean,\n    pca_std,\n    input_channels,\n    n_components=3,\n    cofactor=5.0,\n    chunk_size=100_000\n):\n    \"\"\"\n    Load and preprocess a single IMC image lazily by:\n    - loading image,\n    - normalizing target,\n    - computing residuals from regression,\n    - arcsinh transform,\n    - PCA transform in batches,\n    - Z-score normalization of PCA features,\n    - return PyTorch tensors for model input.\n    \"\"\"\n\n    def to_hwc(img):\n        if img.ndim == 2:\n            img = img[..., None]\n        if img.shape[0] < img.shape[2] and img.shape[0] < img.shape[1]:\n            img = np.transpose(img, (1, 2, 0))\n        return img.astype(np.float32, copy=False)\n\n    img = tifffile.imread(file_path)\n    img = to_hwc(img)\n    H, W, C = img.shape\n    filename = os.path.basename(file_path)\n\n    # Normalize target channel intensities with arcsinh and min-max\n    target_data = img[..., target_idx]\n    y_arc = np.arcsinh(target_data / cofactor).astype(np.float32)\n    y_min, y_max = y_arc.min(), y_arc.max()\n    y_norm = (y_arc - y_min) / (y_max - y_min + 1e-8)\n    y_tensor = torch.from_numpy(y_norm[None, ...]).float()  # (1, H, W)\n\n    # Prepare control channel matrix\n    n_ctrl = len(control_markers_indices)\n    K = len(input_channels)\n    Xc = img[..., control_markers_indices].reshape(-1, n_ctrl)           # (N, n_ctrl)\n    Yin = np.stack([img[..., j].reshape(-1) for j in input_channels], 1)  # (N, K)\n\n    # Stack regression coef and intercept for vectorized prediction\n    B = np.stack([regression_models[j].coef_ for j in input_channels], axis=-1).astype(np.float32)  # (n_ctrl, K)\n    b = np.array([regression_models[j].intercept_ for j in input_channels], dtype=np.float32)       # (K,)\n    b = b.flatten()\n\n    # Predict residuals Y - regression(X)\n    Yhat = Xc @ B + b                                                # (N, K)\n    R = Yin - Yhat\n    R = np.arcsinh(R / cofactor).astype(np.float32)\n\n    # PCA transform in chunks, for low memory\n    N = R.shape[0]\n    Z = np.empty((N, n_components), dtype=np.float32)\n    for s in range(0, N, chunk_size):\n        e = s + chunk_size\n        Z[s:e] = pca.transform(R[s:e])\n\n    # Z-score normalize PCA components\n    Z = (Z - pca_mean) / (pca_std + 1e-8)\n    Z = Z.reshape(H, W, n_components)\n\n    # Convert PCA tensor to channel-first for PyTorch\n    x_tensor = torch.from_numpy(Z).permute(2, 0, 1).contiguous().float()  # (C, H, W)\n\n    return x_tensor, y_tensor, filename\n\n\ndef apply_preprocessing(\n    files,\n    target_idx,\n    control_markers_indices,\n    regression_models,\n    pca,\n    pca_mean,\n    pca_std,\n    input_channels,\n    n_components=3,\n    cofactor=5.0,\n    chunk_size=100_000\n):\n    X_pca_tensor, Y_norm_list, filenames = [], [], []\n\n    for c, f in enumerate(files):\n        if c % int(len(files)/2) == 0 or c == len(files):\n            print(f\"......applying preprocessing: {c+1} / {len(files)}\")\n\n        x_tensor, y_tensor, filename = preprocess_image(\n            f, target_idx, control_markers_indices, regression_models, pca,\n            pca_mean, pca_std, input_channels, n_components, cofactor, chunk_size\n        )\n        X_pca_tensor.append(x_tensor)\n        Y_norm_list.append(y_tensor)\n        filenames.append(filename)\n    return X_pca_tensor, Y_norm_list, filenames\n\ndef pad_collate(batch):\n    \"\"\"\n    Pads all tensors in a batch to the largest size.\n    Also returns original sizes for cropping.\n    \"\"\"\n    X_list, Y_list, original_sizes = zip(*batch)\n\n    max_H = max([x.shape[1] for x in X_list])\n    max_W = max([x.shape[2] for x in X_list])\n\n    padded_X_list = []\n    padded_Y_list = []\n\n    for x_tensor, y_tensor in zip(X_list, Y_list):\n        pad_H = max_H - x_tensor.shape[1]\n        pad_W = max_W - x_tensor.shape[2]\n\n        padded_X = F.pad(x_tensor, (0, pad_W, 0, pad_H))\n        padded_Y = F.pad(y_tensor, (0, pad_W, 0, pad_H))\n\n        padded_X_list.append(padded_X)\n        padded_Y_list.append(padded_Y)\n\n\n    return torch.stack(padded_X_list), torch.stack(padded_Y_list), original_sizes\n\ndef train_model(model, train_loader, val_loader, device, title,\n                epochs=50,\n                lr=1e-4,\n                alpha=0.8,          # weight for Huber; (1 - alpha) for SSIM\n                huber_beta=1.0,\n                grad_clip=1.0,\n                use_amp=True,\n                patience=10,\n                save_path=\"best_model.pth\",\n                dataset=None):\n\n    \"\"\"\n    Loss = alpha * Huber(y_hat, y) + (1 - alpha) * (1 - SSIM(y_hat_norm, y_norm)).\n    - Huber is on intensities.\n    - SSIM is per-image min-max normalized to [0,1].\n    \"\"\"\n\n    model.to(device)\n\n    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.7, patience=5)\n    scaler = GradScaler(enabled=use_amp)\n\n    train_losses, val_losses = [], []\n    best_val, best_state, best_epoch = float(\"inf\"), None, None\n    no_improve = 0\n    epsilon = 1e-8\n\n    for epoch in range(epochs):\n        # Training loop unchanged\n        model.train()\n        running = 0.0\n        for x, y, original_sizes in train_loader:\n            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            B, _, H_padded, W_padded = x.shape  # Padded dimensions\n\n            opt.zero_grad(set_to_none=True)\n            with autocast(device_type='cuda', enabled=use_amp):\n                yhat = model(x)\n\n                hub_sum = 0.0\n                ssim_sum = 0.0\n                for i in range(B):\n                    h, w = original_sizes[i]\n                    y_i = y[i:i+1, :, :h, :w]  # Ground truth (cropped)\n                    yhat_i = yhat[i:i+1, :, :h, :w]  # Prediction (cropped)\n\n                    hub_i = F.smooth_l1_loss(yhat_i, y_i, beta=huber_beta, reduction=\"mean\")\n                    hub_sum += hub_i\n\n                    y_min, y_max = y_i.min().detach(), y_i.max().detach()\n                    yh_min, yh_max = yhat_i.min().detach(), yhat_i.max().detach()\n                    y_n = (y_i - y_min) / (y_max - y_min + epsilon)\n                    yhat_n = (yhat_i - yh_min) / (yh_max - yh_min + epsilon)\n                    ssim_sum += (1.0 - ssim(yhat_n, y_n, data_range=1.0))\n\n                hub_mean = hub_sum / B\n                ssim_mean = ssim_sum / B\n\n                loss = alpha * hub_mean + (1 - alpha) * ssim_mean\n\n            scaler.scale(loss).backward()\n            if grad_clip is not None:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(opt)\n            scaler.update()\n\n            running += loss.item() * B\n\n        train_loss = running / len(train_loader.dataset)\n        train_losses.append(train_loss)\n\n        # Validation loop with robust error checking and batch skipping\n        model.eval()\n        val_running = 0.0\n        valid_batches = 0\n        with torch.no_grad(), autocast(device_type='cuda', enabled=use_amp):\n            for batch_idx, (x, y, original_sizes) in enumerate(val_loader):\n                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n                B, _, H_padded, W_padded = x.shape  # Padded dimensions\n\n                yhat = model(x)\n\n                hub_sum = 0.0\n                ssim_sum = 0.0\n                skip_batch = False\n\n                for i in range(B):\n                    h, w = original_sizes[i]\n                    y_i = y[i:i+1, :, :h, :w]\n                    yhat_i = yhat[i:i+1, :, :h, :w]\n\n                    # Clamp predictions between 0 and 1 for numerical stability\n                    yhat_i = torch.clamp(yhat_i, min=0.0, max=1.0)\n\n                    y_min, y_max = y_i.min(), y_i.max()\n                    yh_min, yh_max = yhat_i.min(), yhat_i.max()\n                    denom_y = (y_max - y_min).abs() + epsilon\n                    denom_yh = (yh_max - yh_min).abs() + epsilon\n\n                    if denom_y < epsilon or denom_yh < epsilon:\n                        #print(f\"Note: validation batch {batch_idx}, sample {i} skipped due to near-constant values\")\n                        skip_batch = True\n                        break\n\n                    y_n = (y_i - y_min) / denom_y\n                    yhat_n = (yhat_i - yh_min) / denom_yh\n\n                    ssim_val = ssim(yhat_n, y_n, data_range=1.0)\n                    if torch.isnan(ssim_val) or torch.isinf(ssim_val):\n                        #print(f\"Note: validation batch {batch_idx}, sample {i} SSIM returned NaN or Inf, skipping batch\")\n                        skip_batch = True\n                        break\n\n                    hub_i = F.smooth_l1_loss(yhat_i, y_i, beta=huber_beta, reduction=\"mean\")\n                    if torch.isnan(hub_i) or torch.isinf(hub_i):\n                        #print(f\"Note: validation batch {batch_idx}, sample {i} Huber loss returned NaN or Inf, skipping batch\")\n                        skip_batch = True\n                        break\n\n                    hub_sum += hub_i\n                    ssim_sum += (1.0 - ssim_val)\n\n                if skip_batch:\n                    continue  # Skip entire batch aggregation\n\n                val_loss_batch = (alpha * hub_sum + (1 - alpha) * ssim_sum) / B\n                if torch.isnan(val_loss_batch) or torch.isinf(val_loss_batch):\n                    #print(f\"Note: validation batch {batch_idx} computed val_loss_batch as NaN or Inf, skipping\")\n                    continue\n\n                val_running += float(val_loss_batch) * B\n                valid_batches += B\n\n        if valid_batches > 0:\n            val_loss = val_running / valid_batches\n        else:\n            val_loss = float('nan')\n\n        val_losses.append(val_loss)\n        sched.step(val_loss)\n\n        print(f\"Epoch {epoch+1:>3d} | Train {train_loss:.3f} | Val {val_loss:.3f}\")\n\n        # ---- Early stopping ----\n        if val_loss + 1e-6 < best_val:\n            best_val, best_epoch = val_loss, epoch + 1\n            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n            no_improve = 0\n            print(f\"  ↳ New best val={val_loss:.3f} at epoch {best_epoch}\")\n            if save_path:\n                torch.save(best_state, save_path)\n                print(f\"  ↳ Saved checkpoint to {save_path}\")\n        else:\n            no_improve += 1\n            print(f\"  ↳ No improvement ({no_improve}/{patience} patience)\")\n            if patience is not None and no_improve >= patience:\n                print(f\"--Early stopping at epoch {epoch+1}. Best epoch was {best_epoch} with val={best_val:.3f}\")\n                break\n\n    # Restore best weights\n    if best_state is not None:\n        model.load_state_dict(best_state)\n        print(f\"--Restored model weights from epoch {best_epoch} (val={best_val:.3f})\")\n\n    # Plot losses\n    plt.figure(figsize=(8, 5))\n    plt.plot(range(1, len(train_losses)+1), train_losses, marker='o', label='Train')\n    plt.plot(range(1, len(val_losses)+1), val_losses, marker='s', label='Val')\n    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title(f'Marker: {title}')\n    plt.legend(); plt.grid(True); plt.show()\n\n    return train_losses, val_losses\n\ndef visualize_prediction(model, dataset, device, indices=None, target_channel=0, name=\"comparison_grid\"):\n    \"\"\"\n    Displays 5 example pairs (Ground truth vs Prediction) side by side.\n    Each row: left = real target image, right = model prediction.\n    \"\"\"\n    model.eval()\n    device = torch.device(device)\n\n    # Select 5 random indices if none provided\n    if indices is None:\n        indices = random.sample(range(len(dataset)), k=min(5, len(dataset)))\n\n    fig, axs = plt.subplots(len(indices), 2, figsize=(8, 4 * len(indices)))\n    if len(indices) == 1:\n        axs = np.expand_dims(axs, axis=0)  # ensure iterable\n\n    for row, idx in enumerate(indices):\n        x_tensor, y_tensor, _ = dataset[idx]\n        x_tensor = x_tensor.unsqueeze(0).to(device)\n        y_tensor = y_tensor.unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            y_pred = model(x_tensor)\n\n        # Convert to numpy, normalize to [0,1]\n        y_true_np = y_tensor.squeeze().cpu().numpy()\n        y_pred_np = y_pred.squeeze().cpu().numpy()\n\n        if y_true_np.ndim == 3:\n            y_true_np = y_true_np[target_channel]\n        if y_pred_np.ndim == 3:\n            y_pred_np = y_pred_np[target_channel]\n\n        y_true_np = (y_true_np - y_true_np.min()) / (y_true_np.max() - y_true_np.min() + 1e-8)\n        y_pred_np = (y_pred_np - y_pred_np.min()) / (y_pred_np.max() - y_pred_np.min() + 1e-8)\n\n        axs[row, 0].imshow(y_true_np, cmap=\"inferno\")\n        axs[row, 0].set_title(f\"Ground Truth (idx {idx})\")\n        axs[row, 0].axis(\"off\")\n\n        axs[row, 1].imshow(y_pred_np, cmap=\"inferno\")\n        axs[row, 1].set_title(\"Prediction\")\n        axs[row, 1].axis(\"off\")\n\n    plt.suptitle(f\"{name}: Real vs Predicted\", fontsize=14)\n    plt.tight_layout(rect=[0, 0, 1, 0.97])\n    plt.show()\n\n\n    plt.savefig(f\"comparison_grid_{name}.png\", dpi=400, bbox_inches=\"tight\")\n    plt.close(fig)\n    print(f\"Saved comparison figure: comparison_grid_{name}.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:22:05.003588Z","iopub.execute_input":"2025-09-25T07:22:05.003827Z","iopub.status.idle":"2025-09-25T07:22:05.249609Z","shell.execute_reply.started":"2025-09-25T07:22:05.003809Z","shell.execute_reply":"2025-09-25T07:22:05.248789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResNet50Encoder(nn.Module):\n    def __init__(self, in_channels=3):\n        super().__init__()\n        backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = backbone.bn1\n        self.relu = backbone.relu\n        self.maxpool = backbone.maxpool\n        self.layer1 = backbone.layer1\n        self.layer2 = backbone.layer2\n        self.layer3 = backbone.layer3\n        self.layer4 = backbone.layer4\n        for param in backbone.parameters():\n            param.requires_grad = False\n\n        #learn params of two last layer\n        for param in self.layer3.parameters():\n            param.requires_grad = True\n            \n        for param in self.layer4.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n        f0 = self.conv1(x)\n        f0 = self.bn1(f0)\n        f0 = self.relu(f0)\n        f0_pool = self.maxpool(f0)\n        f1 = self.layer1(f0_pool)\n        f2 = self.layer2(f1)\n        f3 = self.layer3(f2)\n        f4 = self.layer4(f3)\n        return f0, f1, f2, f3, f4\n\n\nclass UNetDecoder(nn.Module):\n    def __init__(self, out_channels=1, p=0.2):\n        super().__init__()\n        # Stage f4(2048) -> f3 scale (target 1024 ch)\n        self.conv4_1 = nn.Sequential(\n            nn.Conv2d(2048 + 1024, 1024, 3, padding=1, padding_mode='reflect', bias=False),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(p),\n        )\n        self.conv4_2 = nn.Sequential(\n            nn.Conv2d(1024, 1024, 3, padding=1, padding_mode='reflect', bias=False),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(inplace=True),\n        )\n\n        # Stage -> f2 scale (target 512 ch)\n        self.conv3_1 = nn.Sequential(\n            nn.Conv2d(1024 + 512, 512, 3, padding=1, padding_mode='reflect', bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(p),\n        )\n        self.conv3_2 = nn.Sequential(\n            nn.Conv2d(512, 512, 3, padding=1, padding_mode='reflect', bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n        )\n\n        # Stage -> f1 scale (target 256 ch)\n        self.conv2_1 = nn.Sequential(\n            nn.Conv2d(512 + 256, 256, 3, padding=1, padding_mode='reflect', bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(p),\n        )\n        self.conv2_2 = nn.Sequential(\n            nn.Conv2d(256, 256, 3, padding=1, padding_mode='reflect', bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n        )\n\n        # Stage -> combine (f1 + f0) at f1 scale (target 64 ch)\n        self.conv1_1 = nn.Sequential(\n            nn.Conv2d(256 + 256 + 64, 64, 3, padding=1, padding_mode='reflect', bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(p),\n        )\n        self.conv1_2 = nn.Sequential(\n            nn.Conv2d(64, 64, 3, padding=1, padding_mode='reflect', bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n        )\n\n        # Final upsample \n        self.final_up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n\n        # Light head\n        self.up0 = nn.Sequential(\n            nn.Conv2d(64, 32, 3, padding=1, padding_mode='reflect', bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n        )\n        self.final = weight_norm(nn.Conv2d(32, out_channels, 1))\n\n    def forward(self, skips):\n        f0, f1, f2, f3, f4 = skips \n\n        # ---- up4: f4 -> f3 scale ----\n        x = F.interpolate(f4, scale_factor=2, mode='bilinear', align_corners=False)\n        if x.shape[-2:] != f3.shape[-2:]:\n            f3 = F.interpolate(f3, size=x.shape[-2:], mode='bilinear', align_corners=False)\n        x = torch.cat([x, f3], dim=1)\n        x = self.conv4_1(x)\n        x = self.conv4_2(x)\n\n        # ---- up3: -> f2 scale ----\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        if x.shape[-2:] != f2.shape[-2:]:\n            f2 = F.interpolate(f2, size=x.shape[-2:], mode='bilinear', align_corners=False)\n        x = torch.cat([x, f2], dim=1)\n        x = self.conv3_1(x)\n        x = self.conv3_2(x)\n\n        # ---- up2: -> f1 scale ----\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        if x.shape[-2:] != f1.shape[-2:]:\n            f1 = F.interpolate(f1, size=x.shape[-2:], mode='bilinear', align_corners=False)\n        x = torch.cat([x, f1], dim=1)\n        x = self.conv2_1(x)\n        x = self.conv2_2(x)\n\n        # ---- up1: combine f1 & f0 at f1 scale ----\n        # f0 may need resize to f1 scale\n        if f0.shape[-2:] != f1.shape[-2:]:\n            f0 = F.interpolate(f0, size=f1.shape[-2:], mode='bilinear', align_corners=False)\n        x = torch.cat([x, f1, f0], dim=1)\n        x = self.conv1_1(x)\n        x = self.conv1_2(x)\n\n        # ---- final upsample + head ----\n        x = self.final_up(x) \n        x = self.up0(x)\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False) \n        x = torch.sigmoid(self.final(x))\n        return x\n\nclass ResNetUNet(nn.Module):\n    def __init__(self, in_channels=3):\n        super().__init__()\n        self.encoder = ResNet50Encoder(in_channels=in_channels)\n        self.decoder = UNetDecoder()\n\n    def forward(self, x):\n        skips = self.encoder(x)\n        return self.decoder(skips)\n\n# Wrap the lists in a custom Dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, file_list, target_idx, control_markers_indices,\n                 regression_models, pca, pca_mean, pca_std, input_channels,\n                 n_components=3, cofactor=5.0, chunk_size=100_000):\n        self.file_list = file_list\n        self.target_idx = target_idx\n        self.control_markers_indices = control_markers_indices\n        self.regression_models = regression_models\n        self.pca = pca\n        self.pca_mean = pca_mean\n        self.pca_std = pca_std\n        self.input_channels = input_channels\n        self.n_components = n_components\n        self.cofactor = cofactor\n        self.chunk_size = chunk_size\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        file_path = self.file_list[idx]\n        x_tensor, y_tensor, _ = preprocess_image(\n            file_path,\n            self.target_idx,\n            self.control_markers_indices,\n            self.regression_models,\n            self.pca,\n            self.pca_mean,\n            self.pca_std,\n            self.input_channels,\n            self.n_components,\n            self.cofactor,\n            self.chunk_size\n        )\n        H, W = x_tensor.shape[1], x_tensor.shape[2]  \n        return x_tensor, y_tensor, (H, W)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:22:05.294649Z","iopub.execute_input":"2025-09-25T07:22:05.295354Z","iopub.status.idle":"2025-09-25T07:22:05.315005Z","shell.execute_reply.started":"2025-09-25T07:22:05.295335Z","shell.execute_reply":"2025-09-25T07:22:05.314332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main(\n    path: str,\n    target_idx: int,\n    control_markers_indices: list,\n    title: str = \"Unspecified\",\n    n_splits: int = 4,\n    batch_size: int = 2,\n    device: str = None,\n    data_name: str = None,\n    cache_dir: str = \"./cv_cache\",\n    resume: bool = True,\n    n_components: int = 3,\n    cofactor: float = 5.0,\n) -> dict:\n    \"\"\"\n    Fixed-split CV with per-fold caching & resume:\n      - Saves CV split file lists to disk (deterministic KFold with random_state).\n      - Caches preprocessing per fold: regressions + PCA + stats + inputs.\n      - Saves per-fold model checkpoint and per-fold metrics; skips any step that exists.\n      - If the run crashes midway, re-run will continue from the first missing artifact.\n    \"\"\"\n    os.makedirs(cache_dir, exist_ok=True)\n    set_seed(42)\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    extract_folder = path\n    all_files = sorted(\n        [os.path.join(extract_folder, f) for f in os.listdir(extract_folder) if f.endswith(('.tif', '.tiff'))]\n    )\n    if len(all_files) < n_splits:\n        raise ValueError(f\"Number of files ({len(all_files)}) must be >= number of CV splits ({n_splits})\")\n\n    splits_path = os.path.join(cache_dir, f\"cv_splits_{data_name}_{title}_{n_splits}fold.pkl\")\n    if resume and os.path.exists(splits_path):\n        with open(splits_path, \"rb\") as f:\n            splits = pickle.load(f)  # list of tuples: (train_files, val_files)\n        print(f\"[resume] Loaded fixed CV splits from {splits_path}\")\n    else:\n        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n        splits = []\n        for tr_idx, va_idx in kf.split(all_files):\n            train_files = [all_files[i] for i in tr_idx]\n            val_files   = [all_files[i] for i in va_idx]\n            splits.append((train_files, val_files))\n        with open(splits_path, \"wb\") as f:\n            pickle.dump(splits, f)\n        print(f\"[init] Saved fixed CV splits to {splits_path}\")\n\n    rmse_folds, pearson_folds, fid_folds = [], [], []\n    best_rmse, best_fold_idx, best_model_path = None, None, None\n\n    for fold_idx, (train_files, val_files) in enumerate(splits, 1):\n        print(f\"\\n###### Fold {fold_idx}/{n_splits}, marker {title} ######\")\n        torch.cuda.empty_cache()\n\n        fold_tag = f\"{data_name}_{title}_fold{fold_idx}\"\n        preproc_pkl = os.path.join(cache_dir, f\"preproc_{fold_tag}.pkl\")\n        ckpt_path   = os.path.join(cache_dir, f\"model_{fold_tag}.pth\")\n        metrics_js  = os.path.join(cache_dir, f\"metrics_{fold_tag}.json\")\n\n        if resume and os.path.exists(preproc_pkl):\n            with open(preproc_pkl, \"rb\") as f:\n                reg_models, pca, pca_mean, pca_std, input_channels = pickle.load(f)\n            print(f\"[resume] Loaded preprocessing for fold {fold_idx} from {preproc_pkl}\")\n        else:\n            reg_models, pca, pca_mean, pca_std, input_channels = fit_preprocessing(\n                train_files, target_idx, control_markers_indices, n_components=n_components, cofactor=cofactor\n            )\n            with open(preproc_pkl, \"wb\") as f:\n                pickle.dump((reg_models, pca, pca_mean, pca_std, input_channels), f)\n            print(f\"[cache] Saved preprocessing for fold {fold_idx} to {preproc_pkl}\")\n\n        train_dataset = CustomDataset(\n            train_files, target_idx, control_markers_indices,\n            reg_models, pca, pca_mean, pca_std, input_channels,\n            n_components=n_components, cofactor=cofactor\n        )\n        val_dataset = CustomDataset(\n            val_files, target_idx, control_markers_indices,\n            reg_models, pca, pca_mean, pca_std, input_channels,\n            n_components=n_components, cofactor=cofactor\n        )\n\n        train_loader = DataLoader(\n            train_dataset, batch_size=batch_size, shuffle=True,\n            collate_fn=pad_collate, num_workers=2, pin_memory=True\n        )\n        val_loader = DataLoader(\n            val_dataset, batch_size=batch_size, shuffle=False,\n            collate_fn=pad_collate, num_workers=2, pin_memory=True\n        )\n\n        model = ResNetUNet(in_channels=n_components).to(device)\n        # (Optional) channels_last improves memory use:\n        model = model.to(memory_format=torch.channels_last)\n        if torch.cuda.device_count() > 1:\n            model = torch.nn.DataParallel(model)\n\n        if resume and os.path.exists(ckpt_path):\n            # Load weights and skip training\n            state = torch.load(ckpt_path, map_location=device)\n            try:\n                model.load_state_dict(state, strict=True)\n            except Exception:\n                # handle DataParallel prefix\n                if list(state.keys())[0].startswith(\"module.\"):\n                    new_state = OrderedDict((k.replace(\"module.\", \"\"), v) for k, v in state.items())\n                    model.module.load_state_dict(new_state, strict=True) if isinstance(model, torch.nn.DataParallel) else model.load_state_dict(new_state, strict=True)\n                else:\n                    raise\n            print(f\"[resume] Loaded model checkpoint for fold {fold_idx} from {ckpt_path}\")\n        else:\n            # Train from scratch for this fold\n            train_model(\n                model, train_loader, val_loader, device,\n                title=f\"{title} fold {fold_idx}\", dataset=data_name\n            )\n            # Save checkpoint (without DP wrapper if present)\n            state_to_save = model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict()\n            torch.save(state_to_save, ckpt_path)\n            print(f\"[cache] Saved model checkpoint to {ckpt_path}\")\n\n        # ---- (3) Metrics: compute or load ----\n        if resume and os.path.exists(metrics_js):\n            with open(metrics_js, \"r\") as f:\n                fold_metrics = json.load(f)\n            print(f\"[resume] Loaded metrics for fold {fold_idx} from {metrics_js}\")\n            rmse_mean = fold_metrics[\"RMSE_mean_fold\"]\n            pearson_v = fold_metrics[\"Pearson_fold\"]\n            fid_val   = fold_metrics[\"FID_fold\"]\n        else:\n            # Evaluate on val set (same as your original, but streaming FID save)\n            model.eval()\n            rmse_list = []\n            all_y_pred_pixels, all_y_true_pixels = [], []\n\n            # For FID: write resized images on the fly to temp dirs to avoid RAM spikes\n            with tempfile.TemporaryDirectory() as pred_dir, tempfile.TemporaryDirectory() as true_dir:\n                idx_img = 0\n                with torch.no_grad(), autocast(device_type='cuda', enabled=True):\n                    for x, y, _ in val_loader:\n                        x = x.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n                        y = y.to(device, non_blocking=True)\n                        yhat = model(x)\n                        _, _, H, W = y.shape\n                        yhat_cropped = yhat[:, :, :H, :W].clamp(0, 1)\n\n                        # RMSE (per batch)\n                        mse_val = F.mse_loss(yhat_cropped, y).item()\n                        rmse_list.append(np.sqrt(mse_val))\n\n                        # Pearson pixels (accumulate)\n                        yhat_flat = yhat_cropped.detach().cpu().numpy().ravel()\n                        y_flat    = y.detach().cpu().numpy().ravel()\n                        all_y_pred_pixels.append(yhat_flat)\n                        all_y_true_pixels.append(y_flat)\n\n                        # FID: save normalized RGB versions (repeat if single channel)\n                        if yhat_cropped.shape[1] == 1:\n                            y_true_img = y.repeat(1, 3, 1, 1).float()\n                            y_pred_img = yhat_cropped.repeat(1, 3, 1, 1).float()\n                        else:\n                            y_true_img, y_pred_img = y.float(), yhat_cropped.float()\n\n                        # per-image min-max → [0,1]\n                        for b in range(y_true_img.size(0)):\n                            gt = y_true_img[b]\n                            pr = y_pred_img[b]\n                            gt = (gt - gt.min()) / (gt.max() - gt.min() + 1e-8)\n                            pr = (pr - pr.min()) / (pr.max() - pr.min() + 1e-8)\n\n                            gt_np = gt.permute(1,2,0).cpu().numpy()\n                            pr_np = pr.permute(1,2,0).cpu().numpy()\n\n                            # resize to 512×512×3 and save uint8\n                            gt_np = skimage.transform.resize(gt_np, (512, 512, 3), anti_aliasing=True, mode='reflect')\n                            pr_np = skimage.transform.resize(pr_np, (512, 512, 3), anti_aliasing=True, mode='reflect')\n                            gt_u8 = (gt_np * 255).clip(0,255).astype(np.uint8)\n                            pr_u8 = (pr_np * 255).clip(0,255).astype(np.uint8)\n\n                            imageio.imwrite(os.path.join(true_dir, f\"{idx_img}.png\"), gt_u8)\n                            imageio.imwrite(os.path.join(pred_dir, f\"{idx_img}.png\"), pr_u8)\n                            idx_img += 1\n\n                # compute FID\n                fid_val = fid.compute_fid(pred_dir, true_dir, batch_size=batch_size, device=device, mode=\"clean\")\n\n            all_y_pred_pixels = np.concatenate(all_y_pred_pixels)\n            all_y_true_pixels = np.concatenate(all_y_true_pixels)\n            pearson_v, _ = pearsonr(all_y_pred_pixels, all_y_true_pixels)\n            rmse_mean = float(np.mean(rmse_list))\n\n            # cache metrics\n            fold_metrics = {\n                \"marker\": title,\n                \"fold\": fold_idx,\n                \"RMSE_mean_fold\": rmse_mean,\n                \"Pearson_fold\": float(pearson_v),\n                \"FID_fold\": float(fid_val),\n            }\n            with open(metrics_js, \"w\") as f:\n                json.dump(fold_metrics, f, indent=2)\n            print(f\"[cache] Saved fold-{fold_idx} metrics to {metrics_js}\")\n\n        # ---- accumulate / best tracking ----\n        rmse_folds.append(rmse_mean)\n        pearson_folds.append(pearson_v)\n        fid_folds.append(fid_val)\n\n        if best_rmse is None or rmse_mean < best_rmse:\n            best_rmse = rmse_mean\n            best_fold_idx = fold_idx\n            best_model_path = ckpt_path\n\n    # -------------- final summary --------------\n    print(f\"\\n###### Done training {title}! ######\")\n    print(f\"Stats: RMSE = {np.mean(rmse_folds):.3f} ± {np.std(rmse_folds):.3f} | \"\n          f\"Pearson = {np.mean(pearson_folds):.3f} ± {np.std(pearson_folds):.3f} | \"\n          f\"FID = {np.mean(fid_folds):.1f} ± {np.std(fid_folds):.1f}\")\n\n    results = {\n        \"marker\": title,\n        \"RMSE_mean\": float(np.mean(rmse_folds)),\n        \"RMSE_std\": float(np.std(rmse_folds)),\n        \"Pearson_mean\": float(np.mean(pearson_folds)),\n        \"Pearson_std\": float(np.std(pearson_folds)),\n        \"FID_mean(clean)\": float(np.mean(fid_folds)),\n        \"FID_std\": float(np.std(fid_folds)),\n        \"best_fold_idx\": int(best_fold_idx) if best_fold_idx is not None else None,\n        \"best_model_path\": best_model_path,\n        \"splits_path\": splits_path,\n        \"cache_dir\": cache_dir,\n    }\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:25:36.110286Z","iopub.execute_input":"2025-09-25T07:25:36.111116Z","iopub.status.idle":"2025-09-25T07:25:36.127300Z","shell.execute_reply.started":"2025-09-25T07:25:36.111086Z","shell.execute_reply":"2025-09-25T07:25:36.126514Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# load data","metadata":{}},{"cell_type":"code","source":"dataset = \"danenberg2022\"\npath = f\"/kaggle/input/{dataset}\"\ncontrol = [0,37,38] #histone_h3, dna1, dna2\n\nmarker_tasks = [\n    #(\"ki67\", 28, control),\n    #(\"sma\", 1, control), ##training time for sma: 320 minutes\n\n    #(\"CD3\", 13, control),\n    #(\"CD20\", 24, control),\n    #(\"CD45\", 22, control), #training time for CD45: 303 minutes\n    #(\"CD68\", 11, control),\n    \n    (\"pan_cytokeratin\", 35, control), \n    #(\"cytokeratin_5\", 2, control),\n    #(\"cytokeratin_8_18\", 5, control),\n\n    #Keren 2018:\n    #(\"CD11c\", 15, control),\n    #(\"CD16\", 18, control),\n    #(\"hla_dr\", 4, control),\n]\n\nd_results = []\nfor title, target_idx, control_markers_indices in marker_tasks:\n    start_time = time.time()\n    print(f\"Analyzing {dataset} dataset\")\n    \n    res = main(path, target_idx, control_markers_indices, title=title, data_name = dataset)\n    d_results.append(res)\n    \n    df_res = pd.DataFrame([res])\n    df_res.to_csv(f\"{dataset[0]}_results_{title}_5fold.csv\", index=False)\n\n    total_time = time.time() - start_time\n    print(f\"--Total training time for {title}: {total_time/60:.2f} minutes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:25:58.317525Z","iopub.execute_input":"2025-09-25T07:25:58.318132Z","execution_failed":"2025-09-25T10:05:05.085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = \"jackson2020\"\npath = f\"/kaggle/input/{dataset}\"\ncontrol = [0, 33, 34] #histone_h3, dna1, dna2\n\nmarker_tasks = [\n    #(\"ki67\", 25, control),\n    #(\"sma\", 9, control),\n\n    #(\"CD3\", 13, control),\n    #(\"CD20\", 22, control),\n    #(\"CD45\", 20, control),\n    #(\"CD68\", 7, control),\n    \n    (\"pan_cytokeratin\", 31, control),\n    #(\"cytokeratin_5\", 2, control),\n    #(\"cytokeratin_8_18\", 5, control),\n]\n\nj_results = []\nfor title, target_idx, control_markers_indices in marker_tasks:\n    start_time = time.time()\n    print(f\"Analyzing {dataset} dataset\")\n    \n    res = main(path, target_idx, control_markers_indices, title=title, data_name = dataset)\n    \n    df_res = pd.DataFrame([res])\n    df_res.to_csv(f\"{dataset[0]}_results_{title}_5fold.csv\", index=False)\n\n    j_results.append(res)\n\n    total_time = time.time() - start_time\n    print(f\"--Total training time for {title}: {total_time/60:.2f} minutes\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null}]}